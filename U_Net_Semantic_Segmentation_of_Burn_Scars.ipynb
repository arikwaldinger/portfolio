{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arikwaldinger/portfolio/blob/master/U_Net_Semantic_Segmentation_of_Burn_Scars.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RwQu_-VvePY9"
      },
      "source": [
        "# U-Net Semantic Segmentation of Burn Scars\n",
        " **Author: Arik Waldinger**\n",
        "\n",
        "\n",
        "*Acknowledgment Note: This project contains code developed with the assistance of AI tools (Gemini) and incorporates concepts and code structures adapted from materials provided by Professor John Sullivan.*"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Project Ovewrview:\n",
        "\n",
        "\n",
        "##   Summary\n",
        "* This notebook implements a U-Net convolutional neural network for semantic segmentation of burn scars from satellite imagery. The goal is to precisely delineate burned areas, which is a critical task in environmental monitoring and disaster assessment. We leverage the *ChaBuD dataset*, a collection of Sentinel-2 multispectral images, featuring pre- and post-fire observations along with corresponding burn scar masks. The U-Net architecture, known for its effectiveness in medical image segmentation, is adapted here for remote sensing applications, using a pre-trained *ResNet50* encoder to benefit from transfer learning and enhance feature extraction capabilities. The training process incorporates data augmentation (rotation, color jitter) and mixed-precision training for improved generalization and efficiency.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "LamNiqoz1WxK"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "072f8063"
      },
      "source": [
        "## Research Question:\n",
        "\n",
        "What is the optimal combination of backbone encoder and data augmentation strategy for a U-Net model to accurately segment burn scars from Sentinel-2 multispectral imagery?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b2e6b296"
      },
      "source": [
        "### U-Net Backbone Encoder Comparison\n",
        "\n",
        "| Backbone | Description | Test Reasoning | Parameters |\n",
        "| :------- | :---------- | :------------- | :--------- |\n",
        "| **ResNet-18** | Lightweight CNN, shallow residual network | Fast and stable, strong baseline, low compute cost, simple encoder | ≈ 11.7 million |\n",
        "| **EfficientNet-B0** | Very compact model, optimized via neural architecture search | Tests efficiency-focused design, great for low-memory setups| ≈ 11.7 million |\n",
        "| **ResNet-50** | Deeper, more expressive CNN, much larger than ResNet-18 | Tests higher-capacity model,  good for challenging patterns | ≈ 25.6 million |\n",
        "| **SE-ResNeXt50-32x4d** | ResNeXt backbone (grouped convolutions) + Squeeze-and-Excitation attention blocks, multi-path feature processing | Tests modern attention-enhanced CNNs, preforms better on complex textures| ≈ 27 million |"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Burn Scar Machine Learning Motivation\n",
        "\n",
        "**Wildland Fire (Unplanned/Natural):** Mapping these scars is primarily for damage assessment, disaster response, and risk mitigation. It informs emergency services, helps estimate economic losses, guides post-fire recovery, and highlights areas where immediate action is needed to prevent secondary hazards like mudslides or invasive species outbreaks. Machine learning is invaluable here for rapid, real-time assessment over large, often remote, and hazardous areas.\n",
        "\n",
        "**Prescribed Fire (Planned/Controlled):** Mapping burn scars from prescribed burns is focused on monitoring effectiveness, ecological restoration outcomes, and adaptive management. It helps land managers verify if the burn achieved its objectives (e.g., fuel reduction, habitat improvement, pest control), assess burn severity and its impact on vegetation and soil, and refine future burn plans. ML can assist in precisely evaluating burn metrics (e.g., burn completeness, severity) to ensure the desired ecological benefits are realized and unintended consequences are minimized.\n",
        "\n"
      ],
      "metadata": {
        "id": "D8KPU2BtciYZ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZgCgyGree8E"
      },
      "source": [
        "# Install Packages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b88df6be"
      },
      "source": [
        "Installing packages means adding external libraries or modules to your Python environment, which provide pre-written code for specific functionalities. For instance, `torchgeo` is a specialized library that extends PyTorch to handle geospatial data, offering tools and datasets crucial for remote sensing tasks like working with the *ChaBuD dataset* for burn scar segmentation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "BXRHfAb7aWGM"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
        "os.environ['TORCH_USE_CUDA_DSA'] = '1'\n",
        "\n",
        "# Install missing packages\n",
        "!pip install torchviz torchmetrics torchinfo torchgeo\n",
        "\n",
        "# PyTorch Core\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F # Consolidated nnf and F to just F\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "\n",
        "# PyTorch Add-ons\n",
        "from torchviz import make_dot # For model visualization (optional)\n",
        "import torch.optim as optim # Consolidated redundant optim import\n",
        "import torchmetrics # For evaluation metrics\n",
        "from torchinfo import summary # For model summarization\n",
        "from tqdm.auto import tqdm # For progress bars\n",
        "from torchgeo.datasets import ChaBuD # For the dataset\n",
        "import segmentation_models_pytorch as smp # If using models from this library\n",
        "\n",
        "# Data Management\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import rasterio as rio\n",
        "from pathlib import Path # For path manipulation\n",
        "import tempfile # For temporary files\n",
        "\n",
        "# Plotting & Visualization\n",
        "from matplotlib import pyplot as plt\n",
        "from rasterio.plot import show\n",
        "from rasterio.plot import show_hist\n",
        "import seaborn as sns\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt # Duplicate, will keep one"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iasHEEaw-D1k"
      },
      "source": [
        "# Connect to GPU CUDA (Compute Unified Device Architecture)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f2c36f0b"
      },
      "source": [
        "Connecting to a GPU CUDA (Compute Unified Device Architecture) allows your program to utilize the powerful parallel processing capabilities of a graphics processing unit. This significantly accelerates computations, especially for tasks like deep learning and scientific simulations, which can be orders of magnitude faster than running on a CPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ncbnYpZU-C2K"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "torch.manual_seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "531ca7fe"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V3hTHWZmhn6I"
      },
      "source": [
        "Image Settings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l1sMiInshoGb"
      },
      "outputs": [],
      "source": [
        "#Define Burnt\n",
        "BURNT_P = 0.1\n",
        "\n",
        "# Choose an image size\n",
        "#** Do I need to chose an image size (all are 512x512)\n",
        "IMG_SIZE = 512"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xhg2ITI4e-Mv"
      },
      "source": [
        "# Load Burn Scar Data Splits From Torch Geo\n",
        "\n",
        "[ChaBuD Dataset](https://torchgeo.readthedocs.io/en/latest/api/datasets.html#chabud/)\n",
        "Features:\n",
        "\n",
        "\n",
        "* Sentinel-2 multispectral imagery\n",
        "* Inlcudes a binary masks of burned areas\n",
        "* 12 multispectral bands\n",
        "* 356 pairs of pre and post images with 10 m per pixel resolution (512x512 px)\n",
        "* The paired images capture the landscape before and after a fire event\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ed4ac83c"
      },
      "source": [
        "Breakdown of the 12 Sentinel-2 bands and their typical uses:\n",
        "\n",
        "| Band | Band Name            | Resolution (m) | Common Use / Association                     |\n",
        "| :--- | :------------------- | :------------- | :------------------------------------------- |\n",
        "| B01  | Coastal Aeros aerosol| 60             | Atmospheric correction, coastal studies      |\n",
        "| B02  | Blue                 | 10             | Water bodies, urban areas, atmospheric       |\n",
        "| B03  | Green                | 10             | Vegetation reflectance, urban areas          |\n",
        "| B04  | Red                  | 10             | Chlorophyll absorption, urban areas          |\n",
        "| B05  | Vegetation Red Edge 1| 20             | Vegetation health                            |\n",
        "| B06  | Vegetation Red Edge 2| 20             | Vegetation health                            |\n",
        "| B07  | Vegetation Red Edge 3| 20             | Vegetation health                            |\n",
        "| B08  | NIR                  | 10             | Vegetation biomass, water content            |\n",
        "| B8A  | Narrow NIR           | 20             | Improved vegetation indices                  |\n",
        "| B09  | Water Vapour         | 60             | Atmospheric correction                       |\n",
        "| B11  | SWIR 1               | 20             | Soil moisture, burn scars, snow/ice discrimination |\n",
        "| B12  | SWIR 2               | 20             | Vegetation water content, burn scars, geology |"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9de5dcb0"
      },
      "source": [
        "#Remove cache and iniate a fresh downlaod\n",
        "import shutil\n",
        "import os\n",
        "\n",
        "# Define the typical torchgeo cache directory path\n",
        "# This path might vary slightly depending on your environment,\n",
        "# but ~/.cache/torchgeo is a common default.\n",
        "cache_dir = os.path.expanduser(\"~/.cache/torchgeo\")\n",
        "\n",
        "# You might also want to remove a specific dataset folder if you know its name, e.g.,\n",
        "# chabud_dataset_path = os.path.join(cache_dir, \"ChaBuD\")\n",
        "\n",
        "if os.path.exists(cache_dir):\n",
        "    print(f\"Deleting torchgeo cache directory: {cache_dir}\")\n",
        "    shutil.rmtree(cache_dir)\n",
        "    print(\"Cache directory deleted successfully.\")\n",
        "else:\n",
        "    print(f\"Torchgeo cache directory not found at {cache_dir}. Nothing to delete.\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5yuDi8sfYOVZ"
      },
      "outputs": [],
      "source": [
        "#The 804 files have been randomly split into training (2/3) and validation (1/3) directories, each containing the masks, scenes, and index files.\n",
        "from datasets import load_dataset\n",
        "import torchgeo\n",
        "import shutil\n",
        "import os\n",
        "\n",
        "# Define the 12 bands to use\n",
        "selected_bands = ('B01', 'B02', 'B03', 'B04', 'B05', 'B06', 'B07', 'B08', 'B8A', 'B09', 'B11', 'B12')\n",
        "\n",
        "# Define the expected path for the ChaBuD dataset within the torchgeo cache\n",
        "chabud_dataset_path = os.path.join(os.path.expanduser(\"~\"), \".cache\", \"torchgeo\", \"ChaBuD\")\n",
        "\n",
        "# Remove the ChaBuD dataset directory if it exists, to ensure a fresh download\n",
        "if os.path.exists(chabud_dataset_path):\n",
        "    print(f\"Deleting corrupted ChaBuD dataset directory: {chabud_dataset_path}\")\n",
        "    shutil.rmtree(chabud_dataset_path)\n",
        "    print(\"ChaBuD directory deleted successfully. Attempting fresh download...\")\n",
        "else:\n",
        "    print(f\"ChaBuD dataset directory not found at {chabud_dataset_path}. Proceeding with download.\")\n",
        "\n",
        "\n",
        "# Load datasets without any transforms initially\n",
        "tg_train = torchgeo.datasets.ChaBuD(split=\"train\", download=True, bands=selected_bands, transforms=None)\n",
        "tg_val = torchgeo.datasets.ChaBuD(split=\"val\", download=True, bands=selected_bands, transforms=None)\n",
        "#add test split\n",
        "\n",
        "#Chabud Source https://ieeexplore.ieee.org/document/10261881"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example image and mask key of pytrorch tensor from training data smaple 10 from ChaBuD dataset"
      ],
      "metadata": {
        "id": "LtwQebaWKwWE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RZmueSYRcIXX"
      },
      "outputs": [],
      "source": [
        "tg_train[10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "funaf3HNjG2-"
      },
      "source": [
        "# Visualize Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n042MJL0D49p"
      },
      "source": [
        "Import TorchGEO Dataset\n",
        "Then divide into training, validation, and test splits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zux7vFQ2G3FP"
      },
      "outputs": [],
      "source": [
        "dx = 90\n",
        "sample_train = tg_train[dx]\n",
        "\n",
        "# Extract pre-fire and post-fire images\n",
        "\n",
        "# Fetch sample WITHOUT the concatenation transform for visualization\n",
        "temp_tg_train = torchgeo.datasets.ChaBuD(\n",
        "    split=\"train\", download=False, bands=selected_bands, transforms=None\n",
        ")\n",
        "sample_viz = temp_tg_train[dx]\n",
        "\n",
        "# Process pre-fire image\n",
        "img_pre_np = sample_viz['image'][0].permute(1, 2, 0).numpy() # (H, W, C) for pre-fire\n",
        "img_pre_np_normalized = img_pre_np / img_pre_np.max()\n",
        "\n",
        "# Process post-fire image\n",
        "img_post_np = sample_viz['image'][1].permute(1, 2, 0).numpy() # (H, W, C) for post-fire\n",
        "img_post_np_normalized = img_post_np / img_post_np.max()\n",
        "\n",
        "# Get the ground truth mask\n",
        "ground_truth_mask = sample_viz['mask'].squeeze().numpy() # Remove channel dim if present\n",
        "\n",
        "plt.figure(figsize=(18, 6)) # Increased figure size to accommodate 3 plots\n",
        "\n",
        "# Display pre-fire image (RGB)\n",
        "plt.subplot(1, 3, 1) # 1 row, 3 columns, first plot\n",
        "plt.imshow(img_pre_np_normalized[:, :, [0, 1, 2]]) # Displaying the first three bands (R, G, B)\n",
        "plt.title('Train Sample Image (Pre-fire RGB)')\n",
        "plt.axis('off')\n",
        "\n",
        "# Display post-fire image (RGB)\n",
        "plt.subplot(1, 3, 2) # 1 row, 3 columns, second plot\n",
        "plt.imshow(img_post_np_normalized[:, :, [0, 1, 2]]) # Displaying the first three bands (R, G, B)\n",
        "plt.title('Train Sample Image (Post-fire RGB)')\n",
        "plt.axis('off')\n",
        "\n",
        "# Display ground truth mask\n",
        "plt.subplot(1, 3, 3) # 1 row, 3 columns, third plot\n",
        "plt.imshow(ground_truth_mask, cmap='gray') # Displaying the mask\n",
        "plt.title('Ground Truth Mask')\n",
        "plt.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L_fF1HxaFvSL"
      },
      "source": [
        "Example Visualization of a Tile of Index Image: 90\n",
        "(*Pre-burn, Post-Burn, Mask of Burned Area in white*)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2SoGp0vDFouA"
      },
      "source": [
        "Number of Training and Validation Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3su-R4crHCxm"
      },
      "outputs": [],
      "source": [
        "# Get number of samples in train, validation and test\n",
        "print(f\"Number of samples in training dataset: {len(tg_train)}\")\n",
        "print(f\"Number of samples in validation dataset: {len(tg_val)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FbZlQO0-Gr0O"
      },
      "source": [
        "Get Image and mask Dimensions\n",
        "\n",
        "\n",
        "> Input image dimensions: torch.Size([2, 12, 512, 512])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "*   2 Paired Images\n",
        "*   12 Channels for each\n",
        "* 512 pixel width\n",
        "* 512 Pixel Height\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S4d2h3_HGmVA"
      },
      "outputs": [],
      "source": [
        "# Get the IMAGE and MASK dimensions\n",
        "\n",
        "# Use sample_train (already fetched from tg_train) for image dimensions\n",
        "sample_train = tg_train[dx]\n",
        "print(f\"Input image dimensions: {sample_train['image'].shape}\")\n",
        "\n",
        "# Fetch a sample from tg_val to get mask dimensions\n",
        "sample_val = tg_val[0]\n",
        "print(f\"Mask dimensions: {sample_val['mask'].shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KvTJwCoxIpm4"
      },
      "source": [
        "# Normalize Data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Normalize data to improve training stability and speed. Normalizing the dataset can organize data by putting it on the same scale which can make generalization and reaching convergnce easier."
      ],
      "metadata": {
        "id": "FtDhxljH5_s8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OXZXS_I2JKhx"
      },
      "outputs": [],
      "source": [
        "def computeBandStats(dataset, n_chnls):\n",
        "    sum_mean = torch.zeros(n_chnls)\n",
        "    sum_sq_diff = torch.zeros(n_chnls)\n",
        "    count = 0\n",
        "\n",
        "    for i in range(len(dataset)):\n",
        "        img = dataset[i][\"image\"]\n",
        "        # Ensure image has a channel dimension before calculating mean over spatial dims\n",
        "        # If img is (C, H, W), mean over (H, W) is desired\n",
        "        # If img is (H, W), it's a single channel, mean over (H, W) is still fine\n",
        "        if img.ndim == 3: # (C, H, W)\n",
        "            current_mean = torch.nanmean(img.float(), dim=(1, 2))\n",
        "        elif img.ndim == 2: # (H, W) - single channel image\n",
        "            current_mean = torch.nanmean(img.float())\n",
        "        else:\n",
        "            raise ValueError(f\"Unexpected image dimensions: {img.ndim}\")\n",
        "\n",
        "        sum_mean += current_mean\n",
        "        count += 1\n",
        "\n",
        "    mean = sum_mean / count if count > 0 else torch.zeros(n_chnls)\n",
        "\n",
        "    for i in range(len(dataset)):\n",
        "        img = dataset[i][\"image\"]\n",
        "        if img.ndim == 3:\n",
        "            # Expand mean for broadcasting: (C) -> (C, 1, 1)\n",
        "            sum_sq_diff += torch.nanmean((img.float() - mean.unsqueeze(1).unsqueeze(2))**2, dim=(1, 2))\n",
        "        elif img.ndim == 2:\n",
        "            # For single channel, mean is scalar, no need for unsqueeze\n",
        "            sum_sq_diff += torch.nanmean((img.float() - mean[0])**2) # Assuming mean[0] for single channel\n",
        "\n",
        "    std_dev = torch.sqrt(sum_sq_diff / count) if count > 0 else torch.zeros(n_chnls)\n",
        "\n",
        "    return mean, std_dev"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculate Mean and Standard Deviation of Wavelength Values"
      ],
      "metadata": {
        "id": "-sEHBABc66zE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TZGkMnHVIroq"
      },
      "outputs": [],
      "source": [
        "# Define an initial transform to slice the image channels BEFORE mean/std calculation\n",
        "def initial_slicing_transform(sample):\n",
        "    if isinstance(sample, dict) and 'image' in sample:\n",
        "        # ChaBuD images are (T, C, H, W), where T=2 (pre-fire, post-fire)\n",
        "        # Concatenate pre-fire and post-fire images along the channel dimension\n",
        "        if sample['image'].ndim == 4 and sample['image'].shape[0] == 2:\n",
        "            sample['image'] = torch.cat((sample['image'][0, :, :, :], sample['image'][1, :, :, :]), dim=0) # Concatenate pre and post: (2*C, H, W)\n",
        "    return sample\n",
        "\n",
        "# Create a temporary dataset instance to calculate mean and std on sliced data\n",
        "temp_tg_train_for_stats = torchgeo.datasets.ChaBuD(\n",
        "    split=\"train\", download=False, bands=selected_bands, transforms=initial_slicing_transform\n",
        ")\n",
        "\n",
        "# Now compute mean and std, which will operate on already sliced 12-channel images\n",
        "tg_mean, tg_std = computeBandStats(temp_tg_train_for_stats, n_chnls=len(selected_bands) * 2) # Doubled channels\n",
        "print(f\"Mean: {tg_mean}\")\n",
        "print(f\"St Dev: {tg_std}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vXk2JCEnKZvD"
      },
      "source": [
        "# Define Transformations\n",
        "\n",
        "\n",
        "\n",
        "1.   Concatenate pre and post burn bands\n",
        "2.   Apply normalization defined above\n",
        "3. Combine all transforms into one pipeline\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NhCvMiAoKSgL"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torchvision import transforms\n",
        "import random\n",
        "\n",
        "# Define a transform to concatenate pre and post fire images\n",
        "def concatenate_pre_post_bands(sample):\n",
        "    if isinstance(sample, dict) and 'image' in sample:\n",
        "        img = sample['image']\n",
        "        # Ensure image is float for consistency before any ops\n",
        "        img = img.float() # Convert to float early\n",
        "\n",
        "        if img.ndim == 4 and img.shape[0] == 2:\n",
        "            sample['image'] = torch.cat((img[0, :, :, :], img[1, :, :, :]), dim=0) # Result (2*C, H, W)\n",
        "        elif img.ndim == 3 and img.shape[0] == len(selected_bands) * 2:\n",
        "            # Image is already concatenated, likely from previous processing (e.g., for stats)\n",
        "            sample['image'] = img # Just keep it as is\n",
        "        else:\n",
        "            # This covers unexpected dimensions.\n",
        "            raise ValueError(f\"Unexpected image dimensions for concatenation step: {img.shape}\")\n",
        "    return sample\n",
        "\n",
        "# This transform will ONLY normalize the image component of the dictionary\n",
        "class NormalizeDictImage(object):\n",
        "    def __init__(self, mean, std):\n",
        "        self.normalize_transform = transforms.Normalize(mean=mean, std=std)\n",
        "\n",
        "    def __call__(self, sample):\n",
        "        if isinstance(sample, dict) and 'image' in sample:\n",
        "            # The image should already be concatenated and float by this point\n",
        "            sample['image'] = self.normalize_transform(sample['image'])\n",
        "        return sample\n",
        "\n",
        "# --- Define all transformation pipelines ---\n",
        "\n",
        "# Main transform (concatenate + normalize)\n",
        "transform = transforms.Compose([\n",
        "    concatenate_pre_post_bands,\n",
        "    NormalizeDictImage(mean=tg_mean.tolist(), std=tg_std.tolist())\n",
        "])\n",
        "\n",
        "print(\"Transformation classes and pipelines defined successfully without Color Jitter.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQyE_YkvKP-m"
      },
      "source": [
        "Apply normalization transformation to data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ImjP42R8KaVW"
      },
      "outputs": [],
      "source": [
        "# Assign the final normalization transform to the datasets\n",
        "tg_train.transforms = transform\n",
        "tg_val.transforms = transform\n",
        "#tg_test.transforms = transform"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LUJVJfyqojtO"
      },
      "source": [
        "# Data Loader"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Loader (DataLoader)\n",
        "A DataLoader in PyTorch is a utility that sits between your dataset (tg_train and tg_val, which contains images and masks) and training or evaluation loop. Its primary purpose is to efficiently load data in manageable chunks.\n",
        "\n",
        "The batch_size parameter (set to 4 in this cell) defines how many individual samples (in this case, image-mask pairs) are grouped together and processed by the model at once."
      ],
      "metadata": {
        "id": "KWGxlb0z4xmm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ms6GeYXeNHuV"
      },
      "outputs": [],
      "source": [
        "import multiprocessing\n",
        "\n",
        "batch_size = 4 # Reduced batch size to mitigate RAM issues\n",
        "num_cpus = multiprocessing.cpu_count() # Define num_cpus\n",
        "\n",
        "train_dl = DataLoader(tg_train, batch_size=batch_size, shuffle=True, num_workers=0)\n",
        "val_dl = DataLoader(tg_val, batch_size=batch_size, shuffle=False, num_workers=0)\n",
        "\n",
        "print(\"Dataloaders for training and validation created successfully.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4993560"
      },
      "source": [
        "## U-Net Model Architecture and Hyperparameters\n",
        "\n",
        "### Model Architecture:\n",
        "*   **Model Type**: U-Net (from `segmentation_models_pytorch`)\n",
        "*   **Encoder**: ResNet50\n",
        "*   **Encoder Depth**: 5\n",
        "*   **Encoder Weights**: Pre-trained on ImageNet\n",
        "*   **Input Channels**: 24 (12 bands for pre-fire + 12 bands for post-fire images)\n",
        "*   **Output Classes**: 2 (for binary segmentation: Not Burned, Burned)\n",
        "*   **Decoder Channels**: (256, 128, 64, 32, 16)\n",
        "*   **Decoder Batch Normalization**\n",
        "\n",
        "### Hyperparameters:\n",
        "*   **Epochs**: 50\n",
        "*   **Batch Size**: 4\n",
        "*   **Optimizer**: Adam\n",
        "    *   **Learning Rate (lr)**: 0.0002\n",
        "    *   **Weight Decay**: 0.01\n",
        "*   **Learning Rate Scheduler**: StepLR\n",
        "    *   **Step Size**: 10\n",
        "    *   **Gamma**: 0.1\n",
        "*   **Loss Function**: DiceLoss (multiclass mode, 2 classes)\n",
        "*   **Automatic Mixed Precision (AMP)**\n",
        "\n",
        "### Data Augmentation Strategy (Training):\n",
        "*   **Concatenation of Pre/Post Fire Bands**\n",
        "*   **Random Choice Augmentation**: Randomly applies one of the following:\n",
        "    *   No augmentation\n",
        "    *   Random Rotation (degrees=(-10, 10))\n",
        "    *   Color Jitter (brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1) applied only to RGB bands (B02, B03, B04)\n",
        "    *   Random Rotation + Color Jitter\n",
        "*   **Normalization**: Applied after augmentation using calculated mean and standard deviation of all 24 channels.\n",
        "\n",
        "### Data Preparation:\n",
        "*   **Training Dataset Size**: 278 samples\n",
        "*   **Validation Dataset Size**: 78 samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "373575aa"
      },
      "outputs": [],
      "source": [
        "import segmentation_models_pytorch as smp\n",
        "\n",
        "\n",
        "unet = smp.Unet(\n",
        "    encoder_name=\"resnet18\",       # Choose encoder, e.g., \"resnet18\", \"resnet34\", \"vgg16\"\n",
        "    encoder_weights=\"imagenet\",   # Use pre-trained weights for better generalization\n",
        "    in_channels=len(selected_bands) * 2,  # Input channels are now doubled (24 for concatenated pre/post)\n",
        "    classes=2,                      # Output classes (2 for binary segmentation)\n",
        "    decoder_channels=(256, 128, 64, 32, 16), # Default for resnet18 depth 5 (5 decoder blocks)\n",
        "    decoder_use_batchnorm=True      # Match custom UNet's BatchNorm usage\n",
        ")\n",
        "\n",
        "# Print a summary using torchinfo\n",
        "summary(unet, input_size=(batch_size, len(selected_bands) * 2, IMG_SIZE, IMG_SIZE)) # Using global batch_size"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ucb0eL5yTDad"
      },
      "source": [
        "# Define Training and Validation Step"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PJz8aZyeTF5B"
      },
      "outputs": [],
      "source": [
        "import torch # Import torch here\n",
        "import torch.optim as optim # Import optim here\n",
        "from tqdm.auto import tqdm # Import tqdm here\n",
        "import torchmetrics # Import torchmetrics\n",
        "\n",
        "# Import AMP components\n",
        "from torch.amp import autocast, GradScaler # Changed import to torch.amp\n",
        "\n",
        "# Define Training Step\n",
        "def train_step(model: torch.nn.Module, #<- this little bell and whistle creates a data type requirement for the input\n",
        "               dataloader: torch.utils.data.DataLoader,\n",
        "               loss_fn: torch.nn.Module,\n",
        "               optimizer: torch.optim.Optimizer,\n",
        "               metric_class, # Changed to metric_class\n",
        "               epoch: int,\n",
        "               scaler: GradScaler, # Add scaler parameter\n",
        "               device: torch.device = device):\n",
        "\n",
        "  train_loss = 0\n",
        "  model.to(device) # Send model to device\n",
        "  model.train() # activate train mode\n",
        "\n",
        "  # Instantiate a fresh metric for this epoch's training\n",
        "  acc_metric = metric_class(task=\"multiclass\", num_classes=2, average='micro').to(device)\n",
        "\n",
        "  for batch, data in enumerate(tqdm(dataloader, desc=f\"Training Epoch {epoch}\")):\n",
        "\n",
        "    # Send data to GPU\n",
        "    X, y = data['image'].to(device), data['mask'].to(device)\n",
        "    y = y.squeeze(1) # Remove the channel dimension from the mask\n",
        "\n",
        "    # Debug print for input shape\n",
        "    # print(f\"Input X shape to model: {X.shape}\") # Uncomment for debugging\n",
        "\n",
        "    with autocast('cuda'): # Enable mixed precision - changed to autocast('cuda')\n",
        "      # 1. Forward pass\n",
        "      y_pred = model(X) # Model prediction\n",
        "\n",
        "      # 2. Calculate loss\n",
        "      loss = loss_fn(y_pred, y) #Calculate loss\n",
        "\n",
        "    train_loss += loss.item() # Use .item() for scalar loss\n",
        "\n",
        "    # 3. Update accuracy metric\n",
        "    y_pred_arg = y_pred.argmax(dim=1)\n",
        "    acc_metric.update(y_pred_arg, y)\n",
        "\n",
        "    # 4. Optimizer zero grad\n",
        "    optimizer.zero_grad() # Zero gradients\n",
        "\n",
        "    # 5. Loss backward (scale the loss)\n",
        "    scaler.scale(loss).backward() # Backward pass with scaled loss\n",
        "\n",
        "    # 6. Optimizer step (unscale gradients and update weights)\n",
        "    scaler.step(optimizer) # Optimizer step\n",
        "\n",
        "    # 7. Update the scaler for the next iteration\n",
        "    scaler.update()\n",
        "\n",
        "  # Calculate loss and accuracy per epoch and print out what's happening\n",
        "  train_loss /= len(dataloader)\n",
        "  train_acc = acc_metric.compute() # Compute final accuracy for the epoch\n",
        "  acc_metric.reset() # Reset metric state for the next epoch\n",
        "  print(f\"Train loss: {train_loss:.5f} | Train accuracy: {train_acc:.2f}\")\n",
        "\n",
        "  return train_loss, train_acc\n",
        "\n",
        "# Define Validation Step\n",
        "def valtest_step(dataloader: torch.utils.data.DataLoader,\n",
        "                 model: torch.nn.Module,\n",
        "                 loss_fn: torch.nn.Module,\n",
        "                 metric_class, # Changed to metric_class\n",
        "                 epoch: int,\n",
        "                 device: torch.device = device):\n",
        "\n",
        "    # Determine if validation or test split\n",
        "    split = \"Validation\" if dataloader.dataset.split == \"val\" else \"Test\"\n",
        "\n",
        "    loss = 0\n",
        "    model.to(device) # COMPLETE: Send model to device\n",
        "    model.eval() # put model in eval mode\n",
        "\n",
        "    # Instantiate a fresh metric for this epoch's validation\n",
        "    acc_metric = metric_class(task=\"multiclass\", num_classes=2, average='micro').to(device)\n",
        "\n",
        "    # Turn on inference context manager\n",
        "    with torch.inference_mode():\n",
        "        for data in tqdm(dataloader, desc=f\"{split} Epoch {epoch}\"):\n",
        "\n",
        "            # Send data to GPU\n",
        "            X, y = data['image'].to(device), data['mask'].to(device)\n",
        "            y = y.squeeze(1) # Remove the channel dimension from the mask\n",
        "\n",
        "            with autocast('cuda'): # Enable mixed precision for validation - changed to autocast('cuda')\n",
        "                # 1. Forward pass (aka model prediction)\n",
        "                pred = model(X) # COMPLETE: Model prediction\n",
        "\n",
        "                # 2. Calculate loss\n",
        "                loss += loss_fn(pred, y).item() # COMPLETE: Calculate loss, use .item()\n",
        "\n",
        "            # 3. Update accuracy metric\n",
        "            pred_arg = pred.argmax(dim=1)\n",
        "            acc_metric.update(pred_arg, y)\n",
        "\n",
        "        # Adjust metrics and print out what's happening\n",
        "        loss /= len(dataloader)\n",
        "        acc = acc_metric.compute() # Compute final accuracy for the epoch\n",
        "        acc_metric.reset() # Reset metric state for the next epoch\n",
        "        print(f\"{split} loss: {loss:.5f} | {split} accuracy: {acc:.2f}\\n\")\n",
        "\n",
        "    return loss, acc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3DE030WbTkiG"
      },
      "source": [
        "# Define Loss and Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cm_sjBG6TkSe"
      },
      "outputs": [],
      "source": [
        "import segmentation_models_pytorch.losses as smp_losses\n",
        "\n",
        "# Loss and Accuracy\n",
        "# Establish loss function - using DiceLoss for better segmentation performance\n",
        "loss_fn = smp_losses.DiceLoss(mode='multiclass', classes=2)\n",
        "\n",
        "# We will pass the JaccardIndex class and instantiate it inside the training/validation steps\n",
        "JaccardIndexMetric = torchmetrics.JaccardIndex"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6df6eaed"
      },
      "source": [
        "# Cell 6df6eaed was removed as the combination of datasets is no longer needed."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2X9uoQusmK8Q"
      },
      "source": [
        "# Initalize Training Loop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "faba25db"
      },
      "source": [
        "This cell initializes the U-Net model, sets up the optimizer (`Adam` with `lr=0.0002`, `weight_decay=0.01`), and defines the `StepLR` learning rate scheduler (`step_size=10`, `gamma=0.1`). It then executes the training for **50 epochs**, managing mixed-precision training with `GradScaler` and tracking performance metrics for both training and validation sets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b51e5a18"
      },
      "source": [
        "import torch\n",
        "import segmentation_models_pytorch as smp\n",
        "import time # Import the time module\n",
        "\n",
        "# Removed redundant selected_bands definition, using the one from '5yuDi8sfYOVZ'\n",
        "\n",
        "torch.manual_seed(23)\n",
        "unet_smp = smp.Unet(\n",
        "    encoder_name=\"resnet50\",  # <--- CHANGE ENCODER HERE (e.g., \"efficientnet-b0\", \"resnet50\", \"timm-resnest14d\")\n",
        "    encoder_depth=5,         # Set to 5 for resnet18 default depth, consistent with summary\n",
        "    decoder_channels=(256, 128, 64, 32, 16), # Match the decoder channels of depth 5 resnet18 encoder\n",
        "    in_channels=len(selected_bands) * 2,           # Set to the number of input channels in your data (doubled)\n",
        "    classes=2                # Set to the number of classes in your mask\n",
        ")\n",
        "\n",
        "# Establish optimizer\n",
        "# Reduced weight_decay for less aggressive regularization\n",
        "optimizer_smp = optim.Adam(unet_smp.parameters(), lr=0.0002, weight_decay=0.01)\n",
        "\n",
        "# Establish learning rate scheduler\n",
        "scheduler_smp = optim.lr_scheduler.StepLR(optimizer_smp, step_size=10, gamma=0.1) # Add StepLR scheduler\n",
        "\n",
        "# Initialize GradScaler for Automatic Mixed Precision\n",
        "scaler = torch.amp.GradScaler(device='cuda') # Updated to silence FutureWarning\n",
        "\n",
        "# Epochs\n",
        "epochs = 50 # Use the same number of epochs\n",
        "\n",
        "results_smp = {\n",
        "    \"train_loss\": [],\n",
        "    \"train_acc\": [],\n",
        "    \"val_loss\": [],\n",
        "    \"val_acc\": [],\n",
        "    \"epoch_runtime\": [] # New list to store runtime for each epoch\n",
        "    }\n",
        "\n",
        "# Start total training timer\n",
        "total_start_time = time.time()\n",
        "\n",
        "for epoch in tqdm(range(epochs)): #<- this bell and whistle gives you a progress bar while training\n",
        "    print(f\"Epoch: {epoch}\\n---------\")\n",
        "    # Clear CUDA cache at the beginning of each epoch to free up memory\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    epoch_start_time = time.time() # Start epoch timer\n",
        "\n",
        "    # Run train step, passing the metric class\n",
        "    # !!! Using the standard train_dl here !!!\n",
        "    train_loss_smp, train_acc_smp = train_step(model=unet_smp, dataloader=train_dl, loss_fn=loss_fn, optimizer=optimizer_smp, metric_class=JaccardIndexMetric, epoch=epoch, scaler=scaler, device=device)\n",
        "\n",
        "    # Run val step, passing the metric class\n",
        "    val_loss_smp, val_acc_smp = valtest_step(dataloader=val_dl, model=unet_smp, loss_fn=loss_fn, metric_class=JaccardIndexMetric, epoch=epoch, device=device)\n",
        "\n",
        "    epoch_end_time = time.time() # End epoch timer\n",
        "    epoch_duration = epoch_end_time - epoch_start_time\n",
        "\n",
        "    # Update results dictionary\n",
        "    # Ensure all data is moved to CPU and converted to float for storage\n",
        "    results_smp[\"train_loss\"].append(train_loss_smp)\n",
        "    results_smp[\"train_acc\"].append(train_acc_smp.item() if isinstance(train_acc_smp, torch.Tensor) else train_acc_smp)\n",
        "    results_smp[\"val_loss\"].append(val_loss_smp)\n",
        "    results_smp[\"val_acc\"].append(val_acc_smp.item() if isinstance(val_acc_smp, torch.Tensor) else val_acc_smp)\n",
        "    results_smp[\"epoch_runtime\"].append(epoch_duration)\n",
        "\n",
        "    # Step the scheduler\n",
        "    scheduler_smp.step()\n",
        "\n",
        "# End total training timer\n",
        "total_end_time = time.time()\n",
        "total_duration = total_end_time - total_start_time\n",
        "\n",
        "print(f\"\\nTotal training runtime: {total_duration:.2f} seconds\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jWCECmiqi8LA"
      },
      "source": [
        "# Plot Loss and Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W6bvd7kxi-Zy"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create a DataFrame from the results_smp dictionary\n",
        "results_df = pd.DataFrame(results_smp)\n",
        "\n",
        "# Add an 'Epoch' column for better readability\n",
        "results_df['Epoch'] = range(1, len(results_df) + 1)\n",
        "\n",
        "# Reorder columns to have Epoch first, then runtime\n",
        "results_df = results_df[['Epoch', 'epoch_runtime', 'train_loss', 'train_acc', 'val_loss', 'val_acc']]\n",
        "\n",
        "# Format epoch_runtime to two decimal places\n",
        "results_df['epoch_runtime'] = results_df['epoch_runtime'].apply(lambda x: f'{x:.2f}s')\n",
        "\n",
        "# Display the DataFrame\n",
        "display(results_df)\n",
        "\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "# Plot training and validation loss\n",
        "plt.subplot(1, 2, 1) # 1 row, 2 columns, first plot\n",
        "plt.plot(results_df['Epoch'], results_df['train_loss'], label='Train Loss')\n",
        "plt.plot(results_df['Epoch'], results_df['val_loss'], label='Validation Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "# Plot training and validation accuracy\n",
        "plt.subplot(1, 2, 2) # 1 row, 2 columns, second plot\n",
        "plt.plot(results_df['Epoch'], results_df['train_acc'], label='Train IoU Accuracy')\n",
        "plt.plot(results_df['Epoch'], results_df['val_acc'], label='Validation IoU Accuracy')\n",
        "plt.title('Training and Validation IoU Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('IoU Accuracy')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51aebc65"
      },
      "source": [
        "### Interpreting Model Performance: Plots and Quantitative Metrics\n",
        "\n",
        "#### Loss and Accuracy Plots (Training and Validation)\n",
        "\n",
        "*   **Converging Loss**: Both training and validation loss should decrease. Increasing validation loss while training loss decreases indicates **overfitting**.\n",
        "*   **Increasing Accuracy**: Both training and validation IoU accuracy should increase. High losses or low accuracies suggest **underfitting**.\n",
        "\n",
        "#### Quantitative Validation Metrics (Per-Class and Macro Average)\n",
        "\n",
        "These tables offer a numerical evaluation of the model's performance on the validation set for each class.\n",
        "\n",
        "*   **IoU (Intersection over Union) / Jaccard Index**: Measures the overlap between the predicted mask and the ground truth mask. A higher IoU (closer to 1) indicates better overlap and more accurate segmentation. It's calculated as (Area of Overlap) / (Area of Union).\n",
        "*   **Dice Score (F1-score)**: Similar to IoU, the Dice Score also measures similarity. It's often very close in value to IoU and is calculated as (2 * Area of Overlap) / (Sum of Areas). Higher is better.\n",
        "*   **Precision**: Proportion of correctly predicted positive pixels out of all pixels predicted as positive.\n",
        "*   **Recall (Sensitivity)**: Proportion of actual positive pixels that were correctly identified by the model.\n",
        "\n",
        "**Interpreting Per-Class vs. Macro Average:**\n",
        "\n",
        "*   **Per-Class Metrics**: Show performance for specific classes ('Not Burned', 'Burned'). Important for understanding performance on imbalanced classes.\n",
        "*   **Macro Average Metrics**: Provide an overall summary by averaging per-class metrics equally, preventing dominant classes from skewing the total."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Display Validation Predictions of Burned Scars"
      ],
      "metadata": {
        "id": "S7VDKBBYDbMn"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2c1898d2"
      },
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print(\"\\n--- Example Validation Predictions ---\")\n",
        "\n",
        "# --- USER CONFIGURATION START ---\n",
        "# To display specific images, provide a list of indices:\n",
        "indices_to_display = [5, 70, 72] # Always display image indices 5, 70, and 72\n",
        "\n",
        "# The following lines for random selection are no longer needed for fixed indices:\n",
        "# selected_large_burn_indices = random.sample(large_burn_area_indices, min(len(large_burn_area_indices), 4))\n",
        "# indices_to_display = [5] + selected_large_burn_indices\n",
        "# indices_to_display = sorted(list(set(indices_to_display))) # Remove duplicates and sort for consistency\n",
        "\n",
        "# If you want random examples, set indices_to_display to None or an empty list,\n",
        "# and specify the number of random examples you'd like to see:\n",
        "# num_random_examples = 3 # This is now unused given fixed indices\n",
        "# --- USER CONFIGURATION END ---\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "unet_smp.eval()\n",
        "\n",
        "# Get the encoder backbone name\n",
        "encoder_backbone_name = type(unet_smp.encoder).__name__ # Get the class name of the encoder\n",
        "\n",
        "# Prepare the list of indices to iterate over\n",
        "display_indices = indices_to_display # Ensure display_indices is updated with the new list\n",
        "\n",
        "for i, current_idx in enumerate(display_indices): # Loop through the selected indices\n",
        "    print(f\"Displaying example {i+1} (dataset index: {current_idx}):\")\n",
        "    # Select a sample from the validation set\n",
        "    sample = tg_val[current_idx]\n",
        "\n",
        "    # Prepare input for model\n",
        "    image_tensor = sample['image'].unsqueeze(0).to(device) # Add batch dimension, send to device\n",
        "    ground_truth_mask = sample['mask'].squeeze().cpu().numpy() # Remove channel dim, to CPU, to numpy\n",
        "\n",
        "    # Model inference\n",
        "    with torch.no_grad():\n",
        "        with torch.amp.autocast('cuda'): # Use autocast for inference\n",
        "            prediction = unet_smp(image_tensor)\n",
        "    predicted_mask = torch.argmax(prediction, dim=1).squeeze().cpu().numpy()\n",
        "\n",
        "    # Denormalize and separate images for display\n",
        "    image_for_display = sample['image'].cpu() # Move image to CPU and remove batch dimension for processing\n",
        "\n",
        "    # Make sure tg_mean and tg_std are 1D tensors matching image_for_display channels\n",
        "    # Reshape mean and std for broadcasting (C, 1, 1)\n",
        "    mean_tensor = tg_mean.view(-1, 1, 1)\n",
        "    std_tensor = tg_std.view(-1, 1, 1)\n",
        "\n",
        "    # Perform denormalization\n",
        "    image_denormalized = (image_for_display * std_tensor) + mean_tensor\n",
        "\n",
        "    # Scale image values to a displayable range before clamping\n",
        "    # Sentinel-2 reflectance values are typically in a range like 0-10000 or higher.\n",
        "    # For visualization, we need to scale them to 0-1. Dividing by a max value like 3000\n",
        "    # is a common heuristic for natural color composites.\n",
        "    image_denormalized = image_denormalized / 3000.0 # Adjust scaling factor if images are too dark/bright\n",
        "\n",
        "    # Clip values to [0, 1] for proper visualization\n",
        "    image_denormalized = torch.clamp(image_denormalized, 0, 1)\n",
        "\n",
        "    # Separate pre-fire and post-fire images\n",
        "    num_bands = len(selected_bands)\n",
        "    pre_fire_image = image_denormalized[:num_bands]\n",
        "    post_fire_image = image_denormalized[num_bands:]\n",
        "\n",
        "    # Define RGB indices for visualization (B04, B03, B02 are typical R, G, B for Sentinel-2)\n",
        "    # In `selected_bands = ('B01', 'B02', 'B03', 'B04', ...)`, B04 is index 3, B03 is index 2, B02 is index 1\n",
        "    rgb_indices = [3, 2, 1] # Corresponding to B04, B03, B02\n",
        "\n",
        "    # Permute to (H, W, C) for matplotlib and select RGB bands\n",
        "    pre_fire_rgb = pre_fire_image[rgb_indices, :, :].permute(1, 2, 0).numpy()\n",
        "    post_fire_rgb = post_fire_image[rgb_indices, :, :].permute(1, 2, 0).numpy()\n",
        "\n",
        "    # Plotting\n",
        "    plt.figure(figsize=(20, 5)) # Adjust figure size for 4 plots\n",
        "\n",
        "    plt.subplot(1, 4, 1)\n",
        "    plt.imshow(pre_fire_rgb)\n",
        "    plt.title(f'Pre-fire Image (RGB) - Enc: {encoder_backbone_name} - Index {current_idx}')\n",
        "    plt.axis('off')\n",
        "\n",
        "    plt.subplot(1, 4, 2)\n",
        "    plt.imshow(post_fire_rgb)\n",
        "    plt.title(f'Post-fire Image (RGB) - Enc: {encoder_backbone_name} - Index {current_idx}')\n",
        "    plt.axis('off')\n",
        "\n",
        "    plt.subplot(1, 4, 3)\n",
        "    plt.imshow(ground_truth_mask, cmap='gray')\n",
        "    plt.title(f'Ground Truth Mask - Enc: {encoder_backbone_name} - Index {current_idx}')\n",
        "    plt.axis('off')\n",
        "\n",
        "    plt.subplot(1, 4, 4)\n",
        "    plt.imshow(predicted_mask, cmap='gray')\n",
        "    plt.title(f'Predicted Mask - Enc: {encoder_backbone_name} - Index {current_idx}')\n",
        "    plt.axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "25a38300"
      },
      "source": [
        "## Quantitative Performance Metrics on Validation Set\n",
        "\n",
        "\n",
        "*   **Intersection over Union (IoU) / Jaccard Index**: A common metric for segmentation, measuring the overlap between predicted and ground truth masks.\n",
        "*   **Dice Score (F1-score)**: Another popular metric, often correlating closely with IoU, that indicates similarity between two samples.\n",
        "*   **Precision**: The proportion of positive identifications that were actually correct.\n",
        "*   **Recall (Sensitivity)**: The proportion of actual positives that were identified correctly.\n",
        "\n",
        "These metrics are calculated per-class ."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3a8275d8"
      },
      "source": [
        "import torchmetrics\n",
        "import pandas as pd\n",
        "from tqdm.auto import tqdm\n",
        "from torchmetrics import F1Score # Corrected import to use F1Score for Dice\n",
        "\n",
        "# Initialize metrics for binary segmentation\n",
        "# We set average='none' to get per-class metrics\n",
        "num_classes = 2\n",
        "\n",
        "iou_metric = torchmetrics.JaccardIndex(task=\"multiclass\", num_classes=num_classes, average='none').to(device)\n",
        "dice_metric = F1Score(task=\"multiclass\", num_classes=num_classes, average='none').to(device) # Using F1Score for Dice\n",
        "precision_metric = torchmetrics.Precision(task=\"multiclass\", num_classes=num_classes, average='none').to(device)\n",
        "recall_metric = torchmetrics.Recall(task=\"multiclass\", num_classes=num_classes, average='none').to(device)\n",
        "\n",
        "unet_smp.eval()\n",
        "\n",
        "# Loop through the validation dataloader to accumulate metrics\n",
        "with torch.inference_mode():\n",
        "    for data in tqdm(val_dl, desc=\"Calculating Validation Metrics\"):\n",
        "        X, y = data['image'].to(device), data['mask'].to(device)\n",
        "        y = y.squeeze(1)\n",
        "\n",
        "        with torch.amp.autocast('cuda'):\n",
        "            pred = unet_smp(X)\n",
        "\n",
        "        pred_labels = torch.argmax(pred, dim=1)\n",
        "\n",
        "        iou_metric.update(pred_labels, y)\n",
        "        dice_metric.update(pred_labels, y)\n",
        "        precision_metric.update(pred_labels, y)\n",
        "        recall_metric.update(pred_labels, y)\n",
        "\n",
        "# Compute final metrics\n",
        "iou_per_class = iou_metric.compute().cpu().numpy()\n",
        "dice_per_class = dice_metric.compute().cpu().numpy()\n",
        "precision_per_class = precision_metric.compute().cpu().numpy()\n",
        "recall_per_class = recall_metric.compute().cpu().numpy()\n",
        "\n",
        "# Create a DataFrame for better display\n",
        "metrics_df = pd.DataFrame({\n",
        "    'Class': ['Not Burned', 'Burned'],\n",
        "    'IoU': iou_per_class,\n",
        "    'Dice Score': dice_per_class,\n",
        "    'Precision': precision_per_class,\n",
        "    'Recall': recall_per_class\n",
        "})\n",
        "\n",
        "print(\"\\n--- Quantitative Validation Metrics (Per Class) ---\")\n",
        "display(metrics_df)\n",
        "\n",
        "# Optionally, calculate macro average for overall summary\n",
        "macro_iou = torchmetrics.JaccardIndex(task=\"multiclass\", num_classes=num_classes, average='macro').to(device)\n",
        "macro_dice = F1Score(task=\"multiclass\", num_classes=num_classes, average='macro').to(device) # Corrected to use F1Score for Dice\n",
        "macro_precision = torchmetrics.Precision(task=\"multiclass\", num_classes=num_classes, average='macro').to(device)\n",
        "macro_recall = torchmetrics.Recall(task=\"multiclass\", num_classes=num_classes, average='macro').to(device)\n",
        "\n",
        "# Reset and re-compute for macro average (or accumulate within the loop if desired)\n",
        "# For simplicity, re-run inference if necessary, or just use the already updated metrics\n",
        "macro_iou.reset(); macro_dice.reset(); macro_precision.reset(); macro_recall.reset()\n",
        "\n",
        "with torch.inference_mode():\n",
        "    for data in tqdm(val_dl, desc=\"Calculating Macro Average Metrics\"):\n",
        "        X, y = data['image'].to(device), data['mask'].to(device)\n",
        "        y = y.squeeze(1)\n",
        "\n",
        "        with torch.amp.autocast('cuda'):\n",
        "            pred = unet_smp(X)\n",
        "\n",
        "        pred_labels = torch.argmax(pred, dim=1)\n",
        "\n",
        "        macro_iou.update(pred_labels, y)\n",
        "        macro_dice.update(pred_labels, y)\n",
        "        macro_precision.update(pred_labels, y)\n",
        "        macro_recall.update(pred_labels, y)\n",
        "\n",
        "macro_metrics_df = pd.DataFrame({\n",
        "    'Metric': ['IoU', 'Dice Score', 'Precision', 'Recall'],\n",
        "    'Macro Average': [\n",
        "        macro_iou.compute().item(),\n",
        "        macro_dice.compute().item(),\n",
        "        macro_precision.compute().item(),\n",
        "        macro_recall.compute().item()\n",
        "    ]\n",
        "})\n",
        "\n",
        "print(\"\\n--- Quantitative Validation Metrics (Macro Average) ---\")\n",
        "display(macro_metrics_df)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reflection/Discussion\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "wjnVwe_odu8B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generally the 'Burned' class likely appears easier to classify because burn scars have a highly distinct and consistent spectral signature in multispectral imagery, especially when comparing pre- and post-fire conditions. 'Not Burned' class encompasses a much wider and more varied range of land covers, making it inherently more challenging for the model to define and segment precisely.\n",
        "\n",
        "\n",
        "The Resnet 50 encoder performed the highest overall performance. The Resnet 50 boasted a 0.889 validation accuracy (indicating best generalization), 0.223 validation loss, the highest IoU 0.935 (Burned Class) and Dice Score 0.966, indicating the model performed very well at detecting burn scars. ResNet50 likely performed best because its deeper architecture and residual connections allowed it to learn more complex features from the multispectral satellite imagery. Combined with ImageNet pre-trained weights, it could better capture intricate burn scar patterns and generalize effectively to the dataset.\n",
        "\n",
        "\n",
        "\n",
        "ResNet18 likely underperformed due to its limited capacity for learning complex features inherent in multispectral burn scar imagery, showing only a minimal difference from ResNet50. In contrast, the more complex SE-ResNeXt50-32x4d exhibited clear signs of overfitting, with high training accuracy but significantly lower validation performance, especially for the 'Not Burned' class. This suggests it was either too complex for the available data or not optimally tuned for the task, highlighting the potential for bias when pre-tuning specific backbones.\n",
        "\n",
        "\n",
        "\n",
        "Despite trying various data augmentation strategies, including simple rotation, color jitter, a combination of both, and a random choice approach, none improved upon the strong performance of the baseline model, which was trained without any augmentation. Doubling the dataset with rotated copies even led to a slight performance decrease, indicating that general-purpose augmentations might introduce redundant information or noise, hindering rather than helping generalization for this specific burn scar segmentation task. This suggests that the baseline model was already robust, and future augmentation efforts should be highly targeted, carefully controlled, and treated as hyperparameters to address specific data weaknesses without creating unrealistic training examples. It is also possible that there was potential over augmenting or augmenting too many training samples which confused the model and hindered it's a ability to generalize.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "To further enhance the model's performance, several avenues could be explored. Given the small sample size of training validation images, the most impactful improvement would come from expanding the dataset both in terms of quantity and geographically. Additionally, a thorough re-tuning of hyperparameters could uncover more optimal configurations, and exploring advanced data augmentation techniques like elastic deformations, CutMix, or MixUp might improve generalization. Incorporating spectral indices such as NDVI, NDWI, or NDBr directly into the dataset images could also provide richer contextual information for the model. Lastly, experimenting with alternative model architectures, such as a Siamese U-net (with two separate encoders for pre- and post-burn images), and optimizing the loss function with specialized Dice Loss or hybrid approaches could lead to significant gains."
      ],
      "metadata": {
        "id": "DJEdFGsyckoM"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyPVMbI9E7YyuF4ZOu7tMRGM",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}